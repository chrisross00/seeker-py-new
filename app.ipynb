{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project setup, imports, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# approach 2 - PRAW\n",
    "import requests\n",
    "import praw\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "from dateutil import tz\n",
    "from collections import defaultdict\n",
    "\n",
    "# note that CLIENT_ID refers to 'personal use script' and SECRET_TOKEN to 'token'\n",
    "CLIENT_ID = 'Mx4J1DKZ856aT1ug9glOdA'\n",
    "SECRET_TOKEN = 'j0a57wDUiC9TSDQhx1CYNz4jl65Aiw'\n",
    "USERNAME = 'malconst'\n",
    "PASSWORD = '7(>vAn!GqC1\"'\n",
    "USER_AGENT = 'Seeker/0.0.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish authorized Reddit instance\n",
    "reddit = praw.Reddit(\n",
    "    client_id = CLIENT_ID,\n",
    "    client_secret = SECRET_TOKEN,\n",
    "    user_agent = USER_AGENT,\n",
    "    username = USERNAME,\n",
    "    password = PASSWORD\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Reddit\n",
    "PRAW Search doucmentation https://praw.readthedocs.io/en/latest/code_overview/models/subreddit.html?highlight=search#praw.models.Subreddit.search\n",
    "\n",
    "*Important:* the search function in this implementation is a method on the Subreddit object, not Subreddits\n",
    "\n",
    "For now, implemented hashing as described here https://www.codegrepper.com/code-examples/python/python+convert+string+to+hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example of a stored dictionary, to test comparing hashes and accepting/rejecting the search result\n",
    "stored_search_test = {\n",
    "    'searches': [{\n",
    "            'query': 'olivia',\n",
    "            'search_results': [{\n",
    "                'result_id': 'u0wzyl',\n",
    "                'body': {\n",
    "                    'author': 'shirouu-kun',\n",
    "                    'title': '[US-CA] [H] Built Think6.5 v2 Pink Assassin, Think6.5 v2 Solder PCB, 2U Badges, Alpaca and Silent Alpaca Switches, JTK Night Sakura, Latrialum [W] Paypal',\n",
    "                    'created date': '2022-04-10 20:13:00',\n",
    "                    'url': 'https://www.reddit.com/r/mechmarket/comments/u0wzyl/usca_h_built_think65_v2_pink_assassin_think65_v2/',\n",
    "                    'hash': '9278f1a8736b8f34f1cd26a36994a22cd527809a1bdd367f34482f69f32e4143'\n",
    "                }\n",
    "            }]\n",
    "        }, {\n",
    "            'query': '9009',\n",
    "            'search_results': [{\n",
    "                'result_id': 'u08jd9',\n",
    "                'body': {\n",
    "                    'author': 'TradersParad1se',\n",
    "                    'title': '[US-TX] [H] Keycult 2/65 Ocean Grey/ Silver, Tofu65 Grey Aluminum, Bauer 2 PC/Cyan, GMK Modo Light, GMK, GMK Modo2 [W] Paypal, GMK Redacted, GMK Dracula + Nightmode, GMK 9009',\n",
    "                    'created date': '2022-04-09 20:52:43',\n",
    "                    'url': 'https://www.reddit.com/r/mechmarket/comments/u08jd9/ustx_h_keycult_265_ocean_grey_silver_tofu65_grey/',\n",
    "                    'hash': 'f673a1d4ea5b049c407723788a3086235a7e5ebf3ca4d10fac0fb16e53d05312'\n",
    "                }\n",
    "            }]\n",
    "        }, {\n",
    "            'query': 'hammerhead',\n",
    "            'search_results': [{\n",
    "                'result_id': 'u0oded',\n",
    "                'body': {\n",
    "                    'author': 'WestC0ast_BestC0ast',\n",
    "                    'title': '[US-CA] [H] Below Cost Keyboards and GMK Keycaps [W] PayPal',\n",
    "                    'created date': '2022-04-10 12:59:16',\n",
    "                    'url': 'https://www.reddit.com/r/mechmarket/comments/u0oded/usca_h_below_cost_keyboards_and_gmk_keycaps_w/',\n",
    "                    'hash': 'c46e932a561f99d41129267fcd5ba7f5534a61846f7265dd967106ee44e721a3'\n",
    "                }\n",
    "            }]\n",
    "        },\n",
    "        [{\n",
    "            'query': 'olivia',\n",
    "            'search_results': [{\n",
    "                'result_id': 'u0yci3',\n",
    "                'body': {\n",
    "                    'author': Redditor(name = 'danteThrives'),\n",
    "                    'title': '[US-AZ] [H] Paypal or Local Cash [W] Zoom65',\n",
    "                    'created date': '2022-04-10 21:27:34',\n",
    "                    'url': 'https://www.reddit.com/r/mechmarket/comments/u0yci3/usaz_h_paypal_or_local_cash_w_zoom65/',\n",
    "                    'hash': 'f4029b1d8b501516b8f2db16f07fc237f788585ff5c99b2d53756dc52737580c'\n",
    "                }\n",
    "            }]\n",
    "        }]\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit = reddit.subreddit(\"mechmarket\")\n",
    "\n",
    "# get dictionary instead of make it\n",
    "search_dict = {}\n",
    "\n",
    "searches = [] #holder for final storage\n",
    "queries = [\"olivia\",\"9009\",\"hammerhead\"]\n",
    "limit = 1\n",
    "\n",
    "\n",
    "for query in queries:\n",
    "    results = [] #holder for handling each set of search results\n",
    "    for result in subreddit.search(query,sort=\"new\", limit=limit): #https://praw.readthedocs.io/en/latest/code_overview/models/subreddit.html?highlight=search#praw.models.Subreddit.search\n",
    "        # Structure the data before storing - there ahs to be a better way than this... like convering to JSON upstream\n",
    "        result_id = str(result)\n",
    "        to_hash = str(result.author)+ str(result.title)+ str(result.url)\n",
    "        hash_object = hashlib.sha256(to_hash.encode('utf-8'))\n",
    "        hex_dig = hash_object.hexdigest()\n",
    "\n",
    "        #compare hash to existing dictionary\n",
    "\n",
    "        #if store = true\n",
    "        body = {\n",
    "            'author':result.author,\n",
    "            'title':result.title,\n",
    "            'created date': datetime.fromtimestamp(result.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'url':result.url,\n",
    "            'hash':hex_dig\n",
    "        }\n",
    "        results.append({'result_id': result_id, 'body': body}) #just bundle the result_id and body into its own result object and add it to the the results list \n",
    "        \n",
    "    searches.append({'query':query,'search_results':results})\n",
    "search_dict['searches'] = searches #add the final list\n",
    "\n",
    "# store dictionary somewhere\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy of above cell to work out \n",
    "#   pulling from a database (in this case stored_search_test)\n",
    "#   matching new search result hex against database hex\n",
    "#   storing if no match, discarding if match\n",
    "#   updating the search_dict (for current-search results) as usual\n",
    "#   updating the database\n",
    "\n",
    "# could definitely abstract some of the loops to be their own functions\n",
    "\n",
    "## Also, realized you can just run the cell above and work with a memory copy of \"search_dict\" so... did that. Then it \"stopped working\", but since it's just two searches back to back when comparing the hashes with search_dict (because search_dict is from the cell above), so it wouldn't store anything since it all matches\n",
    "## So this wasn't a useful refactor, but does validate that the code works by testing the other case\n",
    "\n",
    "subreddit = reddit.subreddit(\"mechmarket\")\n",
    "\n",
    "# get dictionary instead of make it\n",
    "search_dict = {}\n",
    "\n",
    "searches = [] #holder for final storage\n",
    "queries = [\"olivia\",\"9009\",\"hammerhead\"]\n",
    "limit = 3\n",
    "\n",
    "\n",
    "for query in queries:\n",
    "    results = [] #holder for handling each set of search results\n",
    "    for result in subreddit.search(query,sort=\"new\", limit=limit): #https://praw.readthedocs.io/en/latest/code_overview/models/subreddit.html?highlight=search#praw.models.Subreddit.search\n",
    "        # Structure the data before storing - there ahs to be a better way than this... like convering to JSON upstream\n",
    "        result_id = str(result)\n",
    "        to_hash = str(result.author)+ str(result.title)+ str(result.url)\n",
    "        hash_object = hashlib.sha256(to_hash.encode('utf-8'))\n",
    "        hex_dig = hash_object.hexdigest()\n",
    "\n",
    "        #Evaluate the new hex against the hexes in the database search results\n",
    "        # build a list of all the hashes in the dict, then see if it's in there, if it is. don't store it\n",
    "        hash_list = []\n",
    "        for i in range(len(stored_search_test['searches'])):\n",
    "            for j in range(len(stored_search_test['searches'][i]['search_results'])):\n",
    "                hash_list.append(stored_search_test['searches'][i]['search_results'][j]['body']['hash'])\n",
    "        if hex_dig in hash_list:\n",
    "            print('breaking')\n",
    "            break\n",
    "        else:\n",
    "            print('adding ', hex_dig)\n",
    "            body = {\n",
    "                'author':result.author,\n",
    "                'title':result.title,\n",
    "                'created date': datetime.fromtimestamp(result.created_utc).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'url':result.url,\n",
    "                'hash':hex_dig\n",
    "            }\n",
    "            results.append({'result_id': result_id, 'body': body}) #just bundle the result_id and body into its own result object and add it to the the results list\n",
    "        searches.append({'query':query,'search_results':results})\n",
    "search_dict['searches'].append(searches) #add the final list\n",
    "stored_search_test['searches'].append(searches) # update the stored_dictionary\n",
    "\n",
    "# store dictionary somewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('search_dict after second search; results - ', stored_search_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graveyard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Approach 1 - \n",
    "# import requests\n",
    "# import praw\n",
    "\n",
    "# # note that CLIENT_ID refers to 'personal use script' and SECRET_TOKEN to 'token'\n",
    "# CLIENT_ID = 'Mx4J1DKZ856aT1ug9glOdA'\n",
    "# SECRET_TOKEN = 'j0a57wDUiC9TSDQhx1CYNz4jl65Aiw'\n",
    "# USERNAME = 'malconst'\n",
    "# PASSWORD = '7(>vAn!GqC1\"'\n",
    "# USER_AGENT = 'Seeker/0.0.1'\n",
    "\n",
    "# auth = requests.auth.HTTPBasicAuth(CLIENT_ID, SECRET_TOKEN)\n",
    "\n",
    "# # here we pass our login method (password), username, and password\n",
    "# data = {'grant_type': 'password',\n",
    "#         'username': USERNAME,\n",
    "#         'password': PASSWORD}\n",
    "\n",
    "# # setup our header info, which gives reddit a brief description of our app\n",
    "# headers = {'User-Agent': 'Seeker/0.0.1'}\n",
    "\n",
    "# # send our request for an OAuth token\n",
    "# res = requests.post('https://www.reddit.com/api/v1/access_token',\n",
    "#                     auth=auth, data=data, headers=headers)\n",
    "\n",
    "# # convert response to JSON and pull access_token value\n",
    "# TOKEN = res.json()\n",
    "\n",
    "# # add authorization to our headers dictionary\n",
    "# headers = {**headers, **{'Authorization': f\"bearer {TOKEN}\"}}\n",
    "\n",
    "# res = requests.get(\"https://oauth.reddit.com/r/python/hot\",\n",
    "#                    headers=headers).json()\n",
    "\n",
    "# print(res.url)  # let's see what we get\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "181c2df457f0d00d103b4e25afefeabba3a097333510e9abb596c945bad2abe4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
